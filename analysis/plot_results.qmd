---
title: "Comparing different models for inferring sequency type from metadata"
format:
  html:
    toc: true
    fig-dpi: 200
    html-math-method: katex
---

# Introduction 

Here we compare different models (and preprocessing strategies for working with mostly text-based metadata). Particularly, we compare the two (three) following strategies:

1. Textual (counting words in different columns) and numeric (characterising character-separated numeric columns as their size, minimum, maximum, sum and mean) preprocessing. We then use these values with a random forest classifier or with an XGBoost classifier
2. Plugging the raw data into a CatBoost model.

In terms of development, strategy 1. is characterized by a faster training, but a greater involvement of textual processing. Strategy 2., on the other hand, is characterised by lengthier training but a much easier development as it requires no preprocessing or hyperparameter optimization.

We also test how removing certain features - such as series description (SR), percent phase field of view (% phase FOV) and SAR - affects performance. The reason why we do this is tied with the facts that oftentimes the SR is highly specific of the centre conducting each scan, and that the % phase FOV and SAR are frequently missing from ADC sequences.

In any case we compare results hailing from cross-validation and from a hold-out test set (corresponding to 20% of the full dataset).

```{r data_loading}
#| warning: false

library(tidyverse)
library(knitr)

dir.create("figures",showWarnings=F)

exclusion_match <- c(
    `standard` = "All features",
    `series_description` = "No SR",
    `percent_phase_field_of_view:sar` = "No % phase FOV or SAR",
    `percent_phase_field_of_view:sar:series_description` = "No % phase FOV, SAR or SR"
)
all_metrics <- read_csv("../data_output/metrics.csv") %>%
    mutate(exclusion = factor(exclusion,
                              levels=names(exclusion_match),
                              labels=exclusion_match)) %>%
    mutate(model = factor(
        model,
        levels = c("rf","xgb","catboost"),
        labels = c("Random forest","XGBoost","CatBoost"))) %>%
    group_by(model,exclusion,metric,set) %>%
    mutate(best_fold = fold == fold[which.max(value[split == "cv"])]) %>%
    subset(metric != "support") %>%
    mutate(metric = factor(
        metric,
        levels = c("auc","cm",
                   "precision","recall","f1-score"),
        labels = c("auc","cm",
                   "Precision","Recall","F1-score"))) %>%
    filter(set != "weighted avg") %>%
    mutate(
        set = factor(set,
                     levels = c("adc","dce","dwi",
                                "t2","others",
                                "macro avg"),
                     labels = c("ADC","DCE","DWI",
                                "T2W","Others",
                                "Average"))
    ) 
all_metrics_cv <- all_metrics %>%
    subset(split == "cv")
all_metrics_test <- all_metrics %>%
    subset(split == "test")
```

# Results

## Confusion matrices

### CV

#### As continuous variables

```{r cm_cv_cont,fig.height=8,fig.width=8}
#| label: fig-cm-cv-cont
#| fig-cap: CV confusion matrices.
#| warning: false

all_metrics_cv %>%
    subset(metric == "cm") %>%
    group_by(model,exclusion,true,fold) %>%
    mutate(p = value / sum(value)) %>%
    group_by(model,exclusion,true,pred) %>%
    summarise(p = mean(p),
              value = mean(value)) %>%
    ggplot(aes(x = true,y = pred,
           label = sprintf("%.1f%%\n(%.0f)",p*100,value),
           fill = p)) +
    geom_tile() + 
    geom_text(size = 2) +
    facet_grid(exclusion ~ model) + 
    theme_minimal(base_size = 8) + 
    scale_y_discrete(limits=rev) +
    scale_fill_gradient2(low="lightskyblue1",
                         mid="white",
                         high="goldenrod1",
                         midpoint=0.5,
                         limits = c(0,1),
                         name = "AUC") + 
    xlab("True") + 
    ylab("Predicted") +
    theme(legend.position = "bottom",
          legend.key.height = unit(0.4,"cm")) 
ggsave(filename="figures/cm_cv.pdf",
       height=8,width=8)
```

#### As binned variables (0.02 intervals)

```{r cm_cv_disc,fig.height=8,fig.width=8}
#| label: fig-cm-cv-disc
#| fig-cap: CV confusion matrices (binned).
#| warning: false

all_metrics_cv %>%
    subset(metric == "cm") %>%
    group_by(model,exclusion,true,fold) %>%
    mutate(p = value / sum(value)) %>%
    group_by(model,exclusion,true,pred) %>%
    summarise(p = mean(p),
              value = mean(value)) %>%
    mutate(p_fill = cut(
        p,seq(0,1,by=0.02),
        include.lowest=T)) %>%
    ggplot(aes(x = true,y = pred,
           label = sprintf("%.1f%%\n(%.0f)",p*100,value),
           fill = p_fill)) +
    geom_tile(alpha = 0.6) + 
    geom_text(size = 2) +
    facet_grid(exclusion ~ model) + 
    theme_minimal(base_size = 8) + 
    scale_y_discrete(limits=rev) +
    scale_fill_brewer(palette = "YlOrBr",name = "AUC") +
    xlab("True") + 
    ylab("Predicted") +
    theme(legend.position = "bottom",
          legend.key.height = unit(0.4,"cm")) 
ggsave(filename="figures/cm_cat_cv.pdf",
       height=8,width=8)
```

### Hold-out test-set

#### As continuous variables

```{r cm_ho_cont,fig.height=8,fig.width=8}
#| label: fig-cm-ho-cont
#| fig-cap: Hold-out test-set confusion matrices.
#| warning: false

all_metrics_test %>%
    subset(metric == "cm") %>%
    group_by(model,exclusion,true,fold) %>%
    mutate(p = value / sum(value)) %>%
    group_by(model,exclusion,true,pred) %>%
    summarise(p = mean(p),
              value = median(value)) %>%
    ggplot(aes(x = true,y = pred,
           label = sprintf("%.1f%%\n(%s)",p*100,value),
           fill = p)) +
    geom_tile() + 
    geom_text(size = 2) +
    facet_grid(exclusion ~ model) + 
    theme_minimal(base_size = 8) + 
    scale_y_discrete(limits=rev) +
    scale_fill_gradient2(low="lightskyblue1",
                         mid="white",
                         high="goldenrod1",
                         midpoint=0.5,
                         limits = c(0,1),
                         name = "AUC") + 
    xlab("True") + 
    ylab("Predicted") +
    theme(legend.position = "bottom",
          legend.key.height = unit(0.4,"cm")) 
ggsave(filename="figures/cm_test.pdf",
       height=8,width=8)
```

#### As binned variables (0.02 intervals)

```{r cm_ho_disc,fig.height=8,fig.width=8}
#| label: fig-cm-ho-disc
#| fig-cap: Hold-out test-set confusion matrices (binned).
#| warning: false

all_metrics_test %>%
    subset(metric == "cm") %>%
    group_by(model,exclusion,true,fold) %>%
    mutate(p = value / sum(value)) %>%
    group_by(model,exclusion,true,pred) %>%
    summarise(p = mean(p),
              value = median(value)) %>%
    mutate(p_fill = cut(p,seq(0,1,by=0.02),
                        include.lowest=T)) %>%
    ggplot(aes(x = true,y = pred,
           label = sprintf("%.1f%%\n(%.0f)",p*100,value),
           fill = p_fill)) +
    geom_tile(alpha = 0.6) + 
    geom_text(size = 2) +
    facet_grid(exclusion ~ model) + 
    theme_minimal(base_size = 8) + 
    scale_y_discrete(limits=rev) +
    scale_fill_brewer(palette = "YlOrBr",name = "AUC") +
    xlab("True") + 
    ylab("Predicted") +
    theme(legend.position = "bottom",
          legend.key.height = unit(0.4,"cm")) 
ggsave(filename="figures/cm_cat_test.pdf",
       height=8,width=8)
```

## AUC

### CV

```{r auc_cv,fig.height=2,fig.width=4}
#| label: fig-auc-cv
#| fig-cap: CV AUC.
#| warning: false

all_metrics_cv %>%
    subset(metric == "auc") %>%
    group_by(model,exclusion) %>%
    summarise(vmin = min(value),
              vmax = max(value),
              value = mean(value)) %>%
    ggplot(aes(y = value,x = model,
               ymin = vmin,ymax=vmax,
               label = sprintf("%.2f%%",value*100),
               colour = exclusion)) +
    geom_point(position = position_dodge(0.8)) + 
    geom_linerange(position = position_dodge(0.8)) +
    theme_minimal(base_size = 8) + 
    scale_colour_brewer(type = "qual",palette = 2,
                        name = "Feature subset") +
    xlab("Models") + 
    ylab("AUC") +
    scale_x_discrete(limits = rev) +
    scale_y_continuous(
        labels = function(x) sprintf("%.2f%%",x*100)) +
    theme(legend.key.height = unit(0.2,"cm"),
          legend.key.width = unit(0.2,"cm")) +
    coord_flip() +
    theme(legend.position = "bottom",
          legend.key.height = unit(0.4,"cm")) +
    guides(colour = guide_legend(nrow = 2))
ggsave(filename="figures/auc_cv.pdf",
       height=2,width=4)
```

### Hold-out test set

```{r auc_ho,fig.height=2,fig.width=4}
#| label: fig-auc-ho
#| fig-cap: Hold-out test-set AUC.
#| warning: false

all_metrics_test %>%
    subset(metric == "auc") %>%
    group_by(model,exclusion) %>%
    summarise(vmin = min(value),
              vmax = max(value),
              value = mean(value)) %>%
    ggplot(aes(y = value,x = model,
               ymin = vmin,ymax=vmax,
               label = sprintf("%.2f%%",value*100),
               colour = exclusion)) +
    geom_point(position = position_dodge(0.8)) + 
    geom_linerange(position = position_dodge(0.8)) +
    geom_point(
        data=subset(all_metrics_test,
                    metric == "auc" & best_fold == T),
        aes(x = model,y = value,
            group = exclusion,
            shape = ifelse(best_fold,"Best CV model")),
        inherit.aes=F,
        position = position_dodge(0.8),
        size = 4) +
    scale_shape_manual(values = c("+"),
                       guide = F) +
    theme_minimal(base_size = 8) + 
    xlab("Models") + 
    ylab("AUC") +
    scale_colour_brewer(type = "qual",palette = 2,
                        name = "Feature subset") +
    scale_x_discrete(limits = rev) +
    scale_y_continuous(
        labels = function(x) sprintf("%.2f%%",x*100)) +
    theme(legend.key.height = unit(0.2,"cm"),
          legend.key.width = unit(0.2,"cm")) +
    coord_flip() +
    theme(legend.position = "bottom",
          legend.key.height = unit(0.4,"cm")) +
    guides(colour = guide_legend(nrow = 2))
ggsave(filename="figures/auc_test.pdf",
       height=2,width=4)
```

## Other metrics (precision, recall, F1-score)

### CV

```{r metrics_cv,fig.height=8,fig.width=8}
#| label: fig-metrics-cv
#| fig-cap: CV precision, recall and F1-score.
#| warning: false

M <- c("Precision","Recall","F1-score")

all_metrics_cv %>%
    subset(metric %in% M) %>%
    group_by(model,exclusion,metric,set) %>%
    summarise(vmin = min(value),
              vmax = max(value),
              value = mean(value)) %>%
    ggplot(aes(y = value,x = model,
               ymin = vmin,ymax=vmax,
               label = sprintf("%.2f%%",value*100),
               colour = exclusion)) +
    geom_point(position = position_dodge(0.8)) + 
    geom_linerange(position = position_dodge(0.8)) + 
    theme_minimal(base_size = 8) + 
    xlab("Models") + 
    ylab("Value") +
    facet_grid(set ~ metric) +
    scale_colour_brewer(type = "qual",palette = 2,
                        name = "Feature subset") +
    scale_y_continuous(
        labels = function(x) sprintf("%.2f%%",x*100)) +
    theme(legend.key.height = unit(0.2,"cm"),
          legend.key.width = unit(0.2,"cm"),
          panel.background = element_rect(
            fill = NA,colour = "black")) +
    coord_flip() +
    theme(legend.position = "bottom",
          legend.key.height = unit(0.4,"cm")) +
    guides(colour = guide_legend(nrow = 2))
ggsave(filename="figures/metrics_cv.pdf",
       height=8,width=8)
```

### Hold-out test-set

```{r metrics_ho,fig.height=8,fig.width=8}
#| label: fig-metrics-ho
#| fig-cap: Hold-out test-set precision, recall and F1-score.
#| warning: false

all_metrics_test %>%
    subset(metric %in% M) %>%
    group_by(model,exclusion,metric,set) %>%
    summarise(vmin = min(value),
              vmax = max(value),
              value = mean(value)) %>%
    ggplot(aes(y = value,x = model,
               ymin = vmin,ymax=vmax,
               label = sprintf("%.2f%%",value*100),
               colour = exclusion)) +
    geom_point(position = position_dodge(0.8)) + 
    geom_linerange(position = position_dodge(0.8)) + 
    geom_point(
        data=subset(all_metrics_test,
                    metric %in% M & best_fold == T),
        aes(x = model,y = value,
            group = exclusion,
            shape = ifelse(best_fold,"Best CV model")),
        inherit.aes=F,
        position = position_dodge(0.8),
        size = 3) +
    scale_shape_manual(values = c("+"),
                       guide = F) +
    theme_minimal(base_size = 8) + 
    xlab("Models") + 
    ylab("Value") +
    facet_grid(set ~ metric) +
    scale_colour_brewer(type = "qual",palette = 2,
                        name = "Feature subset") +
    scale_y_continuous(
        labels = function(x) sprintf("%.2f%%",x*100)) +
    theme(legend.key.height = unit(0.2,"cm"),
          legend.key.width = unit(0.2,"cm"),
          panel.background = element_rect(
            fill = NA,colour = "black")) +
    coord_flip() +
    theme(legend.position = "bottom",
          legend.key.height = unit(0.4,"cm")) +
    guides(colour = guide_legend(nrow = 2))
ggsave(filename="figures/metrics_test.pdf",
       height=8,width=8)
```

# Discussion

The consensus is relatively easy to reach - CatBoost is, by far, the best performing model out of the two. The advantages are considerable - the optimization is simpler and no preprocessing is required. While it takes longer to train, one can be sure that the results will be hard to dispute.